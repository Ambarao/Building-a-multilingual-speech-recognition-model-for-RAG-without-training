# Step 1: Setup Environment
!pip install git+https://github.com/openai/whisper.git
!pip install transformers
!pip install torch
!pip install librosa
!pip install datasets
!pip install faiss-cpu

# Step 2: Upload the Audio File
from google.colab import files

# Upload the audio file
uploaded = files.upload()

# Extract the filename from the uploaded files
audio_path = list(uploaded.keys())[0]
print(f"Uploaded file: {audio_path}")

# Step 3: Load and Preprocess the Audio File
import librosa

# Load and preprocess the audio file in chunks
def load_audio_in_chunks(audio_path, chunk_duration=30):
    audio, sr = librosa.load(audio_path, sr=16000)
    chunk_samples = int(chunk_duration * sr)

    for start_sample in range(0, len(audio), chunk_samples):
        yield audio[start_sample:start_sample + chunk_samples], sr

chunks = list(load_audio_in_chunks(audio_path))

print(f"Loaded {len(chunks)} chunks from the audio file.")

# Step 4: Transcribe Each Chunk
import whisper

# Load the pre-trained multilingual Whisper model with FP32
model = whisper.load_model("medium", device="cpu")

transcriptions = []

for i, (audio_chunk, sr) in enumerate(chunks):
    result = model.transcribe(audio_chunk, fp16=False)  # Ensure FP32 is used
    transcriptions.append(result["text"])
    print(f"Transcription of chunk {i+1}/{len(chunks)}: {result['text']}")

# Combine all transcriptions
full_transcription = " ".join(transcriptions)
print("Full Transcription:", full_transcription)

# Step 5: Translate the Transcription
from transformers import MarianMTModel, MarianTokenizer

# Load the translation model and tokenizer
model_name = "Helsinki-NLP/opus-mt-en-de"  # Change to your target language if needed
tokenizer = MarianTokenizer.from_pretrained(model_name)
translation_model = MarianMTModel.from_pretrained(model_name)

# Translate the transcription
inputs = tokenizer(full_transcription, return_tensors="pt", padding=True)
translated_tokens = translation_model.generate(**inputs)
translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
print("Translated Text:", translated_text)

# Step 6: Integrate with RAG
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration

# Load the RAG tokenizer, retriever, and model
tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-nq")
retriever = RagRetriever.from_pretrained("facebook/rag-token-nq", index_name="exact", use_dummy_dataset=True)
rag_model = RagSequenceForGeneration.from_pretrained("facebook/rag-token-nq", retriever=retriever)

# Use the translated text as the query
query = translated_text
input_ids = tokenizer(query, return_tensors="pt").input_ids

# Generate the response
generated_ids = rag_model.generate(input_ids=input_ids)
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
print("RAG Response:", response)
